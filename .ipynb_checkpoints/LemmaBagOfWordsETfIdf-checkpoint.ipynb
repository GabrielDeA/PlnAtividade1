{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 1. Importação de Bibliotecas e Preparação do Ambiente\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import string\n",
    "\n",
    "# Download de recursos da NLTK (necessário apenas na primeira execução)\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc29f68d8607f0b",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "with open(r'JsonSoup/JsonRacas/lemma/dwarf_lemmatized_structured.json', 'r') as file_object:\n",
    "        jsonCarregado = json.load(file_object)\n",
    "\n",
    "def extract_strings(obj):\n",
    "    if isinstance(obj, str):\n",
    "        yield obj\n",
    "    elif isinstance(obj, dict):\n",
    "        for v in obj.values():\n",
    "            yield from extract_strings(v)\n",
    "    elif isinstance(obj, list):\n",
    "        for item in obj:\n",
    "            yield from extract_strings(item)\n",
    "\n",
    "# Turn into a list (corpus)\n",
    "corpusAux = list(extract_strings(jsonCarregado))\n",
    "\n",
    "corpus = [\n",
    "    s.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    for s in corpusAux\n",
    "]\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "# Carrega a lista de stopwords original da NLTK\n",
    "english_stop_words_original = set(stopwords.words('english'))\n",
    "english_stop_words_original.update(string.punctuation)\n",
    "\n",
    "stop_words_stemmed = [stemmer.stem(word) for word in english_stop_words_original]\n",
    "\n",
    "def stem_tokenizer(text):\n",
    "    tokens = text.split()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "bow_vec = CountVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words=stop_words_stemmed,  # Usa a lista de stopwords corrigida\n",
    "    tokenizer=stem_tokenizer\n",
    ")\n",
    "X_bow = bow_vec.fit_transform(corpus)\n",
    "df_bow = pd.DataFrame(X_bow.toarray(), columns=bow_vec.get_feature_names_out())\n",
    "\n",
    "print(\"Matriz Documento-Termo (BoW):\")\n",
    "df_bow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aa4043fa34f402",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words=stop_words_stemmed, # Usa a lista de stopwords corrigida\n",
    "    tokenizer=stem_tokenizer\n",
    ")\n",
    "X_tfidf = tfidf_vec.fit_transform(corpus)\n",
    "df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vec.get_feature_names_out())\n",
    "\n",
    "print(\"Matriz Documento-Termo (TF-IDF):\")\n",
    "df_tfidf.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fae9368e8717fb",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def search_and_rank(query, vectorizer, X_corpus, corpus, method_name):\n",
    "    \"\"\"\n",
    "    Vetoriza uma query, calcula a similaridade com o corpus e exibe os resultados.\n",
    "    \"\"\"\n",
    "    q_vec = vectorizer.transform([query])\n",
    "    sim_scores = cosine_similarity(q_vec, X_corpus).ravel()\n",
    "    rank = np.argsort(sim_scores)[::-1]\n",
    "\n",
    "    print(f\"Top-3 Similares para a Query (usando {method_name}):\")\n",
    "    for i in rank[:3]:\n",
    "        if sim_scores[i] > 0.01: # Apenas mostra se houver alguma similaridade\n",
    "            print(f\"  Doc{i+1} (score={sim_scores[i]:.3f}): {corpus[i]}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "query = 'I am a dwarf and im digging a hole'\n",
    "print(f\">> Executando busca para a query: '{query}'\\n\")\n",
    "search_and_rank(query, bow_vec, X_bow, corpus, \"BoW\")\n",
    "search_and_rank(query, tfidf_vec, X_tfidf, corpus, \"TF-IDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272ad30ccc2beb90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T00:06:51.689179Z",
     "start_time": "2025-09-17T00:06:50.314251Z"
    }
   },
   "outputs": [],
   "source": [
    "# 7. Visualizações\n",
    "\n",
    "# a) Heatmap da Similaridade de Cossenos entre Documentos\n",
    "# Primeiro, calculamos a matriz de similaridade (documento x documento) usando a matriz TF-IDF\n",
    "print(\"Calculando a matriz de similaridade de cossenos...\")\n",
    "sim_matrix = cosine_similarity(X_tfidf, X_tfidf)\n",
    "\n",
    "# Para melhor visualização, criamos um DataFrame com os rótulos corretos\n",
    "doc_labels = [f\"Doc{i+1}\" for i in range(len(corpus))]\n",
    "df_similarity = pd.DataFrame(sim_matrix, index=doc_labels, columns=doc_labels)\n",
    "\n",
    "# Agora, geramos o heatmap a partir dessa matriz de similaridade\n",
    "plt.figure(figsize=(11, 9))\n",
    "sns.heatmap(\n",
    "    df_similarity,\n",
    "    annot=True,          # Exibe os valores de similaridade nas células\n",
    "    cmap=\"Blues\",        # Mapa de cores em tons de azul\n",
    "    fmt=\".2f\"            # Formata os números com duas casas decimais\n",
    ")\n",
    "plt.title(\"Heatmap de Similaridade de Cossenos entre Documentos\")\n",
    "plt.xlabel(\"Documentos\")\n",
    "plt.ylabel(\"Documentos\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626ee00015ec4d6b",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# b) Visualização dos Documentos em 2D com PCA\n",
    "pca = PCA(n_components=2)\n",
    "# Usamos .toarray() para converter a matriz esparsa para uma matriz densa\n",
    "X_tfidf_pca = pca.fit_transform(X_tfidf.toarray())\n",
    "\n",
    "plt.figure(figsize=(13, 10))\n",
    "plt.scatter(X_tfidf_pca[:, 0], X_tfidf_pca[:, 1], c='blue', alpha=0.7, s=100)\n",
    "plt.title(\"Visualização 2D dos Vetores de Documentos (TF-IDF + PCA)\")\n",
    "plt.xlabel(\"Componente Principal 1\")\n",
    "plt.ylabel(\"Componente Principal 2\")\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "# Adiciona anotações para cada ponto\n",
    "for i, _ in enumerate(corpus):\n",
    "    plt.annotate(f\"Doc{i+1}\", (X_tfidf_pca[i, 0], X_tfidf_pca[i, 1]),\n",
    "                 xytext=(8, -8), textcoords='offset points', fontsize=9,\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"yellow\", alpha=0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb173c0d0d084c6f",
   "metadata": {},
   "source": [
    "## Análise  \n",
    "\n",
    "### Cluster de Similaridade (Documentos 3, 16 e 20)  \n",
    "No canto direito, há um cluster de 3 linhas extremamente semelhantes:  \n",
    "\n",
    "- **Doc3:** \"Ability Score Increase. Your Constitution score increases by 2.\"  \n",
    "- **Doc16:** \"Ability Score Increase. Your Wisdom score increases by 1.\"  \n",
    "- **Doc20:** \"Ability Score Increase. Your Strength score increases by 2.\"  \n",
    "\n",
    "Isso mostra que o modelo consegue localizar frases similares com sucesso.  \n",
    "\n",
    "---\n",
    "\n",
    "### Outro caso de alta similaridade (Doc10 e Doc21)  \n",
    "- **Doc10:** \"Dwarven Combat Training. You have proficiency with the battleaxe, handaxe, light hammer, and warhammer.\"  \n",
    "- **Doc21:** \"Dwarven Armor Training. You have proficiency with light and medium armor.\"  \n",
    "\n",
    "---\n",
    "\n",
    "### Caso de baixa similaridade (Doc11 e Doc12)  \n",
    "- **Doc11:** \"Darkvision. Accustomed to life underground, you have superior vision in dark and dim conditions. You can see in dim light within 60 feet of you as if it were bright light, and in darkness as if it were dim light. You can't discern color in darkness, only shades of gray.\"  \n",
    "- **Doc12:** \"Dwarven Resilience. You have advantage on saving throws against poison, and you have resistance against poison damage.\"  \n",
    "\n",
    "Apesar de estarem próximos no espaço visual, a similaridade entre eles foi apenas **0.12** no heatmap, o que demonstra que realmente não há forte relação entre os conteúdos em duas dimenções.\n",
    "Houvessem mais, provavelmente estes valores estariam distantes em outros eixos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b637bb3-bff8-41bb-bd78-0939b6698d59",
   "metadata": {},
   "source": [
    "***Usando os tokens com lemming***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3729ba93-a9e8-420a-83c2-a12bcee7061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'JsonSoup/JsonRacas/lemma/dwarf_lemmatized_structured.json', 'r') as file_object:\n",
    "        jsonCarregado = json.load(file_object)\n",
    "\n",
    "def extract_strings(obj):\n",
    "    if isinstance(obj, str):\n",
    "        yield obj\n",
    "    elif isinstance(obj, dict):\n",
    "        for v in obj.values():\n",
    "            yield from extract_strings(v)\n",
    "    elif isinstance(obj, list):\n",
    "        for item in obj:\n",
    "            yield from extract_strings(item)\n",
    "\n",
    "# Turn into a list (corpus)\n",
    "corpusAux = list(extract_strings(jsonCarregado))\n",
    "\n",
    "corpus2 = [\n",
    "    s.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    for s in corpusAux\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef68773-3b09-4b82-bcde-4a58a5d1f49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Heatmap da Similaridade de Cossenos entre Documentos\n",
    "# Primeiro, calculamos a matriz de similaridade (documento x documento) usando a matriz TF-IDF\n",
    "print(\"Calculando a matriz de similaridade de cossenos...\")\n",
    "sim_matrix = cosine_similarity(X_tfidf, X_tfidf)\n",
    "\n",
    "# Para melhor visualização, criamos um DataFrame com os rótulos corretos\n",
    "doc_labels = [f\"Doc{i+1}\" for i in range(len(corpus2))]\n",
    "df_similarity = pd.DataFrame(sim_matrix, index=doc_labels, columns=doc_labels)\n",
    "\n",
    "# Agora, geramos o heatmap a partir dessa matriz de similaridade\n",
    "plt.figure(figsize=(11, 9))\n",
    "sns.heatmap(\n",
    "    df_similarity,\n",
    "    annot=True,          # Exibe os valores de similaridade nas células\n",
    "    cmap=\"Blues\",        # Mapa de cores em tons de azul\n",
    "    fmt=\".2f\"            # Formata os números com duas casas decimais\n",
    ")\n",
    "plt.title(\"Heatmap de Similaridade de Cossenos entre Documentos\")\n",
    "plt.xlabel(\"Documentos\")\n",
    "plt.ylabel(\"Documentos\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# b) Visualização dos Documentos em 2D com PCA\n",
    "pca = PCA(n_components=2)\n",
    "# Usamos .toarray() para converter a matriz esparsa para uma matriz densa\n",
    "X_tfidf_pca = pca.fit_transform(X_tfidf.toarray())\n",
    "\n",
    "plt.figure(figsize=(13, 10))\n",
    "plt.scatter(X_tfidf_pca[:, 0], X_tfidf_pca[:, 1], c='blue', alpha=0.7, s=100)\n",
    "plt.title(\"Visualização 2D dos Vetores de Documentos (TF-IDF + PCA)\")\n",
    "plt.xlabel(\"Componente Principal 1\")\n",
    "plt.ylabel(\"Componente Principal 2\")\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "# Adiciona anotações para cada ponto\n",
    "for i, _ in enumerate(corpus2):\n",
    "    plt.annotate(f\"Doc{i+1}\", (X_tfidf_pca[i, 0], X_tfidf_pca[i, 1]),\n",
    "                 xytext=(8, -8), textcoords='offset points', fontsize=9,\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"yellow\", alpha=0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940ea40a-7692-45a9-8c6c-1b41882becea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
